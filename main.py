# main.py

from llm import get_llm
from vector_store import create_vectorstore_from_texts
from retriever import get_retriever

# ì˜ˆì‹œìš© í…ìŠ¤íŠ¸ (ê³¼ì œìš©)
texts = [
    "ê³ í˜ˆì••ì—ëŠ” ë‚˜íŠ¸ë¥¨ ì„­ì·¨ë¥¼ ì¤„ì´ê³  ì¹¼ë¥¨ì´ ë§ì€ ì‹í’ˆì„ ì„­ì·¨í•´ì•¼ í•©ë‹ˆë‹¤.",
    "ì¹¼ë¥¨ì´ ë§ì€ ìŒì‹ìœ¼ë¡œëŠ” ë°”ë‚˜ë‚˜, ì‹œê¸ˆì¹˜, ì•„ë³´ì¹´ë„ ë“±ì´ ìˆìŠµë‹ˆë‹¤.",
    "ìš´ë™ê³¼ ì‹ì´ìš”ë²•ì€ ê³ í˜ˆì•• ê´€ë¦¬ì— ì¤‘ìš”í•©ë‹ˆë‹¤."
]

# ë²¡í„°ìŠ¤í† ì–´ ìƒì„± ë° ê²€ìƒ‰ê¸° ìƒì„±
vectorstore = create_vectorstore_from_texts(texts)
retriever = get_retriever(vectorstore)

# LLM ì—°ê²°
llm = get_llm()

# ì‚¬ìš©ì ì§ˆë¬¸
query = input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”: ")

# ë¬¸ì„œ ê²€ìƒ‰ + ë‹µë³€ ìƒì„±
docs = retriever.get_relevant_documents(query)
context = "\n".join([doc.page_content for doc in docs])

prompt = f"""ë„ˆëŠ” ê±´ê°• ìƒë‹´ ì „ë¬¸ê°€ì•¼. ì•„ë˜ ë¬¸ì„œë¥¼ ì°¸ê³ í•´ì„œ ì§ˆë¬¸ì— ë‹µë³€í•´ì¤˜.

[ë¬¸ì„œ]
{context}

[ì§ˆë¬¸]
{query}

[ë‹µë³€]"""

response = llm.predict(prompt)
print("\nğŸ“¢ ë‹µë³€:", response)
